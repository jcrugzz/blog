# Replication and inefficient data structures

Now to preface this, run the following curl commands or just follow along with the given data. This shows the state of all the currently known replicas as of the late evening of February 23rd.

```
curl -ks https://isaacs.iriscouch.com/registry
curl -s https://fullfatdb.npmjs.com/registry
curl -s http://registry.npmjs.org.au
curl -s http://registry.npmjs.eu
curl -s http://npm.strongloop.com
curl -ks https://registry.nodejitsu.com
```
![](https://i.cloudup.com/PjA3bW4BQJ.png)
![](https://i.cloudup.com/IdL9XB7gwn.png)
![](https://i.cloudup.com/5ZzKbynNAo.png)
You may notice a couple things here from the JSON blobs:

1. There is a _huge_ disparity between `disk_size` and `data_size` in some of the above cases.
2. All of the replicas are either behind or somehow have more documents than FullFatDB.

Many of you may have seen the [new npm architecture][npm-arch] and I for one was pretty excited about the claim of having much less garbage[1]. The size of the FullFatDB database is a mere ~100GB which seems like it would make it much more reasonable to replicate from. Unfortunately this doesn’t seem to quite be the case. The more notable example is the original registry `npm`, `isaacs.iriscouch.com` compared to the new FullFatDB that it replicates from. The `data_size` of this `npm` has a bit larger starting point (~190GB), **but since the switch in architecture, this number has more than _doubled_ in size to over ~650GB!**

In traditional CouchDB replication, the `disk_size` and `data_size` are usually _very_ close in number. Sometimes inefficiencies in the underlying data structure cause them to differ _slightly_ but nothing that should cause this kind of behavior. This is all a result of [changes to the architecture of npm.][npm-arch] There is a larger post suggesting [how to solve this problem][nodejitsu-post], but lets take a look at how CouchDB structures data to understand how this could all happen.

## To the data structures!

For those of you that don't know, [CouchDB][couch] uses a data-structure called a [B+ tree][b-plus] to store documents and any changes in the future made to that document. Each new [revision][couch-rev] of a document is stored using this particular data structure and each document in the whole database instance is also traversed as this tree structure. In the case of `npm`, each new update or `publish` of a package, traverses the database B-tree for the document, increments this revision number and appends it to the end of the B-tree. Now if I haven't lost you yet, lets get into why this matters here.

## Conflicts in CouchDB

If you are not familiar with this concept in CouchDB, I highly recommend that you read [Jan Lehnardt's][jan] write up on them [here][conflicts]. To give you the brief synopsis, a CouchDB conflict is caused when multiple writes are completed concurrently on the same document. Unlike serial writes where each revision is based off the prior writes, CouchDB has no automatic way of knowing if you intend to overwrite or merge the contents of the document. These conflicts can happen between replicated pairs of databases both accepting writes, clustered CouchDB or using the replicator API with the `?new_edits=false` parameter. This query parameter is used internally in replication to ensure that masterless peer-to-peer replication can always occur, even if it causes a conflict. Now this feature is poorly documented because it can have unintended negative consequences if it is not used _exactly_ how the replicator uses it internally. These negative consequences are exactly the case npm is running into with the formation of FullFatDB. Using this feature _forces_ whatever revision is given to become the new latest revision. In this case it is used to force the FullFatDB to have the same exact revisions[2] as the SkimDB.

## Creating inefficient B+ trees in npm

Pause here. Do you understand B+ trees and conflicts? Good. We can now begin to understand exactly why we get this data bloat. Each time a document is published to `npm`, it is inserted into the new SkimDB system, has its tarball plucked off and is then reformed into the FullFatDB we have been talking about. Since this database is formed via conflicts, the b-tree that represents the entire database becomes more difficult to traverse due to increased depth at the various document nodes and introduces possible [write amplification][write-amp]. While this shouldn’t pose a severe problem on its own, this extra inefficiency in combination with the current `npm` data model causes the current implementation of the CouchDB `_replicator` to work _much_ harder than it needs to. The result of this process is that the `_replicator`will not be able to easily grab the update it needs, _especially_ in the case of attachments. When the `_replicator` cannot easily replicate a document, it will keep retrying until it can grab the full document. What I have seen from sifting through lines and lines of CouchDB logs is that it cannot find specific attachment stubs of a document in the peer database. This causes this retry process to get stuck in an infinite loop in some cases and create all kinds of garbage[1] data. Since replication treats documents as atomic units, if a document cannot completely replicate, it must retry the replication from the beginning. If we had already replicated most of the document and attachments when this occurs, we just introduce more garbage[1] into the database. This is the type of behavior seen by `isaacs.iriscouch.com` leading to this prompt [reporting](https://twitter.com/npmjs/status/436898866747559936) when one of our disks reached capacity.

## What can we do?

This post is not a call to action but an educational piece. The goal here is to begin to think deeper into the data store we use and how we can get the most out of it. I have proposed a solution to this problem in [another post here][nodejitsu-post]. I hope that these technical facts make it clear that there is a problem and a way for us to fix it together.


_Footnotes_
-----------------------------
1. Garbage is wasted resources like IO throughput and storage space that don't actually contribute to the final durable storage and retrieval of the data.

2. The revision hash in CouchDB is always based on the content of the hash

[couch]: https://couchdb.apache.org/
[b-plus]: https://couchdb.apache.org/
[couch-rev]: http://guide.couchdb.org/draft/api.html#revisions
[jan]: https://github.com/janl
[conflicts]: http://writing.jan.io/2013/12/19/understanding-couchdb-conflicts.html
[write-amp]: https://en.wikipedia.org/wiki/Write_amplification
[npm-arch]: http://blog.npmjs.org/post/75707294465/new-npm-registry-architecture
[nodejitsu-post]: http://blog.nodejitsu.com/sustaining-open-replication-and-scaling-npm/
